# -*- coding: utf-8 -*-
"""TYPE2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zWWbbZ1VlKrpQgcAY_KXH44QlwPWIA0M
"""

   pip install "unstructured[all-docs]" pillow pydantic lxml pillow matplotlib

  sudo apt-get install poppler-utils
  sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn
  pip install unstructured-pytesseract
  pip install tesseract-ocr

from unstructured.partition.pdf import partition_pdf

"/content/extracted_data"
"/content/cj.pdf"

raw_pdf_elements=partition_pdf(
    filename="/content/cj.pdf",
    strategy="hi_res",
    extract_images_in_pdf=True,
    extract_image_block_types=["Image", "Table"],
    extract_image_block_to_payload=False,                  # 	Saves extracted images to a folder instead of returning them as data.
    extract_image_block_output_dir="extracted_data",
    )

raw_pdf_elements

Header=[]
Footer=[]
Title=[]
NarrativeText=[]
Text=[]
ListItem=[]
for element in raw_pdf_elements:
  if "unstructured.documents.elements.Header" in str(type(element)):
            Header.append(str(element))
  elif "unstructured.documents.elements.Footer" in str(type(element)):
            Footer.append(str(element))
  elif "unstructured.documents.elements.Title" in str(type(element)):
            Title.append(str(element))
  elif "unstructured.documents.elements.NarrativeText" in str(type(element)):
            NarrativeText.append(str(element))
  elif "unstructured.documents.elements.Text" in str(type(element)):
            Text.append(str(element))
  elif "unstructured.documents.elements.ListItem" in str(type(element)):
            ListItem.append(str(element))

img=[]
for element in raw_pdf_elements:
  if "unstructured.documents.elements.Image" in str(type(element)):
            img.append(str(element))

tab=[]
for element in raw_pdf_elements:
  if "unstructured.documents.elements.Table" in str(type(element)):
            tab.append(str(element))

tab

img

Text

  pip install langchain_core
  pip install langchain-google-genai
  pip install langchain
  pip install chromadb

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

"""Table summary"""

prompt_text = """You are an assistant tasked with summarizing tables for retrieval. \
    These summaries will be embedded and used to retrieve the raw table elements. \
    Give a concise summary of the table that is well optimized for retrieval. Table:{element} """

prompt = ChatPromptTemplate.from_template(prompt_text)

import os
from google.colab import userdata
GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

model = ChatGoogleGenerativeAI(model="gemini-2.0-flash",google_api_key=GEMINI_API_KEY,)
llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro-exp-03-25",google_api_key=GEMINI_API_KEY,)

summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

table_summaries = []
table_summaries = summarize_chain.batch(tab, {"max_concurrency": 5})
table_summaries

"""Text summary"""

prompt_text = """You are an assistant tasked with summarizing text for retrieval. \
    These summaries will be embedded and used to retrieve the raw text elements. \
    Give a concise summary of the table or text that is well optimized for retrieval.text: {element} """

prompt = ChatPromptTemplate.from_template(prompt_text)

text_summaries = []
text_summaries = summarize_chain.batch(Text, {"max_concurrency": 5})
text_summaries

"""Image Summary

"""

import base64
from langchain_core.messages import HumanMessage

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")




import os
import base64
import google.generativeai as genai
from PIL import Image
import io
import time
import random

genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def resize_and_encode_image(image_path, max_size=(256, 256), quality=70):
    try:
        img = Image.open(image_path).convert('L') #convert to grayscale
        img.thumbnail(max_size)
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format="JPEG", quality=quality)
        return base64.b64encode(img_byte_arr.getvalue()).decode("utf-8")
    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        return None

def image_summarize(base64_image, prompt):
    model = genai.GenerativeModel('gemini-2.0-flash')
    max_retries = 5
    retry_delay = 2
    max_wait = 60
    for attempt in range(max_retries):
        try:
            response = model.generate_content([prompt, {"mime_type": "image/jpeg", "data": base64_image}])
            return response.text
        except Exception as e:
            if "exceeded your quota" in str(e).lower() or "token" in str(e).lower() or "429" in str(e):
                print(f"Retry attempt {attempt + 1} failed: {e}. Retrying in {retry_delay} seconds.")
                time.sleep(retry_delay + random.random())
                retry_delay *= 2
                retry_delay = min(retry_delay, max_wait)
            else:
                raise e
    return "Summary generation failed after multiple retries."

def generate_img_summaries(path):
    img_base64_list = []
    image_summaries = []
    prompt = "Summarize the image for retrieval in 2 sentences."

    for img_file in sorted(os.listdir(path)):
        if img_file.endswith(".jpg"):
            img_path = os.path.join(path, img_file)
            base64_image = resize_and_encode_image(img_path)

            if base64_image:
                img_base64_list.append(base64_image)
                image_summaries.append(image_summarize(base64_image, prompt))

    return img_base64_list, image_summaries

fpath="/content/extracted_data/"

img_base64_list, image_summaries = generate_img_summaries(fpath)

img_base64_list

image_summaries

  pip install langchain_community

import uuid #Generates unique doc IDs

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore  #to store metadata
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings

  pip install -U langchain-chroma

from langchain_chroma import Chroma

def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):
    store = InMemoryStore()
    id_key = "doc_id"

    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key=id_key,
    )

    def add_documents(retriever, doc_summaries, doc_contents):
        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
        summary_docs = [
            Document(page_content=s, metadata={id_key: doc_ids[i]})
            for i, s in enumerate(doc_summaries)
        ]
        retriever.vectorstore.add_documents(summary_docs)
        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))

    # âœ… Actually call the function here
    if text_summaries:
        add_documents(retriever, text_summaries, texts)

    if table_summaries:
        add_documents(retriever, table_summaries, tables)

    if image_summaries:
        add_documents(retriever, image_summaries, images)

    return retriever

vectorstore = Chroma(
    collection_name="mm_rag", embedding_function=GoogleGenerativeAIEmbeddings(model="models/embedding-001",google_api_key=GEMINI_API_KEY,)
)

retriever_multi_vector_img = create_multi_vector_retriever(
    vectorstore,
    text_summaries,
    Text,
    table_summaries,
    tab,
    image_summaries,
    img_base64_list,
)

retriever_multi_vector_img

import io
import re

from IPython.display import HTML, display
from PIL import Image

def plt_img_base64(img_base64):
    """Disply base64 encoded string as image"""
    # Create an HTML img tag with the base64 string as the source
    image_html = f''
    # Display the image by rendering the HTML
    display(HTML(image_html))

plt_img_base64(img_base64_list[1])

image_summaries[1]

def looks_like_base64(sb):
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None

def is_image_data(b64data):
    image_signatures = {
        b"\xFF\xD8\xFF": "jpg",
        b"\x89\x50\x4E\x47\x0D\x0A\x1A\x0A": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False

def resize_base64_image(base64_string, size=(128, 128)):

    img_data = base64.b64decode(base64_string)
    img = Image.open(io.BytesIO(img_data))

    resized_img = img.resize(size, Image.LANCZOS)

    buffered = io.BytesIO()
    resized_img.save(buffered, format=img.format)

    return base64.b64encode(buffered.getvalue()).decode("utf-8")

def split_image_text_types(docs):

    b64_images = []
    texts = []

    for doc in docs:
        if isinstance(doc, Document):
            doc = doc.page_content
        if looks_like_base64(doc) and is_image_data(doc):
            doc = resize_base64_image(doc, size=(1300, 600))
            b64_images.append(doc)
        else:
            texts.append(doc)

    return {"images": b64_images, "texts": texts}

def img_prompt_func(data_dict):

    formatted_texts = "\n".join(data_dict["context"]["texts"])
    messages = []

    if data_dict["context"]["images"]:
        for image in data_dict["context"]["images"]:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)

    text_message = {
        "type": "text",
        "text": (
            "You are a helpful assistant.\n"
            "You will be given a mixed info(s) .\n"
            "Use this information to provide relevant information to the user question. \n"
            f"User-provided question: {data_dict['question']}\n\n"
            "Text and / or tables:\n"
            f"{formatted_texts}"
        ),
    }
    messages.append(text_message)
    return [HumanMessage(content=messages)]

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

def multi_modal_rag_chain(retriever):

    chain = (
        {
            "context": retriever | RunnableLambda(split_image_text_types),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )

    return chain

chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)

chain_multimodal_rag

query = "Why We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end?"
docs = retriever_multi_vector_img.invoke(query)

docs

query="Open-Domain QA Test Scores. For TQA,\
left column uses the standard test set for Open-\
Domain QA, right column uses the TQA-Wiki\
test set. See Appendix D for further details."

docs = retriever_multi_vector_img.invoke(query)

docs

plt_img_base64(docs)

query="can you explain me this Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\
in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved."

query1="what is the image about"

chain_multimodal_rag.invoke(query1)

