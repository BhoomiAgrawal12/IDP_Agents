# -*- coding: utf-8 -*-
"""TYPE3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KIA57s_fPrcqate13azVX_2u45H-FRBq
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-vector-stores-lancedb
# %pip install google-gemini-embeddings
# %pip install llama-index-multi-modal-llms-gemini
# %pip install llama-index-readers-file

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama_index

# Commented out IPython magic to ensure Python compatibility.
# %pip install lancedb
# %pip install moviepy
# %pip install pytube
# %pip install pydub
# %pip install SpeechRecognition
# %pip install ffmpeg-python
# %pip install soundfile
# %pip install torch torchvision
# %pip install matplotlib scikit-image
# %pip install ftfy regex tqdm

from moviepy.editor import VideoFileClip
from pathlib import Path
import speech_recognition as sr
from pytube import YouTube
from pprint import pprint
from PIL import Image
import matplotlib.pyplot as plt

import os
from google.colab import userdata
GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

import os
print(os.getcwd())

video_url="https://www.youtube.com/watch?v=CtsRRUddV2s&ab_channel=VisuallyExplained"

output_video_path="/content/video_data/"

output_folder = "/content/mixed_data/"
output_audio_path = "/content/mixed_data/output_audio.wav"

!mkdir mixed_data

filepath=output_video_path + "input_vid.webm"
print(filepath)

# Commented out IPython magic to ensure Python compatibility.
# %pip install yt-dlp

# from pytube import YouTube

# def download_video(url, output_path):
#     try:
#         yt = YouTube(url)
#         metadata = {"Author": yt.author, "Title": yt.title, "Views": yt.views}
#         yt.streams.get_highest_resolution().download(
#             output_path=output_path, filename="input_vid.mp4"
#         )
#         return metadata
#     except Exception as e:
#         print("Error downloading video:", e)
#         return None
import yt_dlp

def download_video(url, output_path):
    ydl_opts = {
        'outtmpl': f'{output_path}/input_vid.%(ext)s',
        'format': 'bestvideo+bestaudio/best',
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        metadata = {
            "Author": info.get('uploader'),
            "Title": info.get('title'),
            "Views": info.get('view_count')
        }
    return metadata

from moviepy.editor import VideoFileClip
def video_to_images(video_path,output_folder):
  clip=VideoFileClip(video_path)
  clip.write_images_sequence(
      os.path.join(output_folder,"frame%04d.png"),fps=0.2
  )

def video_to_audio(video_path,output_audio_path):
  clip=VideoFileClip(video_path)
  audio=clip.audio
  audio.write_audiofile(output_audio_path)

import base64
import google.generativeai as genai

# Commented out IPython magic to ensure Python compatibility.
# %pip install google-cloud-speech

from google.cloud import speech

def audio_to_text(audio_file):
    client = speech.SpeechClient()
    with open(audio_file, 'rb') as f:
        audio = speech.RecognitionAudio(content=f.read())
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.FLAC,
        sample_rate_hertz=16000,
        language_code="en-US",
        enable_automatic_punctuation=True,
    )

    response = client.recognize(config=config, audio=audio)

    for result in response.results:
        print("Transcript:", result.alternatives[0].transcript)

!apt-get install ffmpeg -y

output_video_path

metadata_vid = download_video(video_url, output_video_path)

video_url

!python -m pytube --clear-cache

metadata_vid

video_to_images(filepath,output_folder)

filepath

output_audio_path

video_to_audio(filepath,output_audio_path)

!ffmpeg -i /content/mixed_data/output_audio.wav -c:a flac /content/mixed_data/output_audio.flac

!ls /content/mixed_data/

output_audio_path1="/content/mixed_data/output_audio.flac"

!pip install moviepy torchaudio torch

# # Extract audio
# video_clip = VideoFileClip(output_video_path)
# video_clip.audio.write_audiofile(output_audio_path, codec='flac')
# video_clip.close()
# import torch
# import torchaudio
# from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq

# # Step 2: Load Whisper model
# device = "cuda" if torch.cuda.is_available() else "cpu"

# processor = AutoProcessor.from_pretrained("openai/whisper-large-v3")
# model = AutoModelForSpeechSeq2Seq.from_pretrained("openai/whisper-large-v3").to(device)

# # Step 3: Load and process the audio


# # Load audio
# speech_array, sampling_rate = torchaudio.load(output_audio_path)

# # If sampling_rate is not 16000, resample it (Whisper expects 16000 Hz)
# if sampling_rate != 16000:
#     resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
#     speech_array = resampler(speech_array)
#     sampling_rate = 16000

# # Flatten the audio to 1D
# speech = speech_array.squeeze()

# # Preprocess
# inputs = processor(speech, sampling_rate=sampling_rate, return_tensors="pt").to(device)

# # Step 4: Generate transcription
# with torch.no_grad():
#     generated_ids = model.generate(**inputs)

# transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

# print("Transcription:", transcription)

# Import required libraries
import torch
import torchaudio
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq

# Step 1: Load Whisper model
device = "cuda" if torch.cuda.is_available() else "cpu"

processor = AutoProcessor.from_pretrained("openai/whisper-large-v3")
model = AutoModelForSpeechSeq2Seq.from_pretrained("openai/whisper-large-v3").to(device)

# Step 2: Load and process the audio
# Load audio
speech_array, sampling_rate = torchaudio.load(output_audio_path1)

# If sampling_rate is not 16000, resample it (Whisper expects 16000 Hz)
if sampling_rate != 16000:
    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
    speech_array = resampler(speech_array)
    sampling_rate = 16000

# Squeeze and convert to numpy
speech = speech_array.squeeze(0).numpy()

# Preprocess
inputs = processor(speech, sampling_rate=sampling_rate, return_tensors="pt").to(device)

# Step 3: Generate transcription
with torch.no_grad():
    generated_ids = model.generate(**inputs)

# Step 4: Decode
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

print("Transcription:", transcription)

























text_data=audio_to_text(output_audio_path1)

print(text_data)

with open(output_folder + "output_text.txt", "w") as file:
        file.write(text_data)
print("Text data saved to file")
file.close()

os.remove(output_audio_path)
print("Audio file removed")